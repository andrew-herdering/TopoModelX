{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Dynamic Hypergraph Neural Network (DHGNN)\n",
    "\n",
    "In this notebook, we will create and train a two-step message passing network in the hypergraph domain. We will use a benchmark dataset, MUTAG (from the TUDataset), to train the model to perform binary classification at the level of the h.\n",
    "\n",
    "### The Neural Network:\n",
    "\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "A convolution from edges to edges using a cohomology message passing scheme:\n",
    "\n",
    "ðŸŸ¥ $\\quad m_{y \\rightarrow x}^{(r' \\rightarrow r)} = M^t_{\\mathcal{C}}(h_{x}^{t,(r)}, h_y^{t,(r')}, x, y)$ \n",
    "\n",
    "ðŸŸ§ $\\quad m_x^{(r' \\rightarrow r)}  = AGG_{y \\in \\mathcal{C}(x)} m_{y \\rightarrow x}^{(r' \\rightarrow r)}$ \n",
    "\n",
    "ðŸŸ© $\\quad m_x^{(r)} = m_x^{(r' \\rightarrow r)}$ \n",
    "\n",
    "ðŸŸ¦ $\\quad h_{x}^{t+1,(r)} = U^{t,(r)}(h_{x}^{t,(r)}, m_{x}^{(r)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031).\n",
    "\n",
    "### The Task:\n",
    "\n",
    "We train this model to perform entire complex classification on [`MUTAG` from the TUDataset](https://paperswithcode.com/dataset/mutag). This dataset contains:\n",
    "- 188 samples of chemical compounds represented as graphs,\n",
    "- with 7 discrete node features.\n",
    "\n",
    "The task is to predict the mutagenicity of each compound on Salmonella typhimurium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from toponetx import SimplicialComplex\n",
    "from topomodelx.nn.hypergraph.dhgnn_layer import DHGNNLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, MUTAG, a benchmark dataset for graph classification. We then lift each graph into our domain of choice, a hypergraph.\n",
    "\n",
    "We will also retrieve:\n",
    "- input signal on the edges for each of these hypergraphs, as that will be what we feed the model in input\n",
    "- the binary label associated to the hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:53.022151550Z",
     "start_time": "2023-06-01T16:14:52.949636599Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = TUDataset(root=\"/tmp/MUTAG\", name=\"MUTAG\", use_edge_attr=True)\n",
    "dataset = dataset[:20]\n",
    "hg_list = []\n",
    "x_1_list = []\n",
    "y_list = []\n",
    "for graph in dataset:\n",
    "    hg = SimplicialComplex(to_networkx(graph)).to_hypergraph()\n",
    "    hg_list.append(hg)\n",
    "    x_1 = torch.chunk(graph.edge_attr, 2, dim=0)[1]\n",
    "    x_1_list.append(x_1)\n",
    "    y_list.append(int(graph.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on each simplicial complex. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:54.320559691Z",
     "start_time": "2023-06-01T16:14:54.310717495Z"
    }
   },
   "outputs": [],
   "source": [
    "incidence_1_list = []\n",
    "for hg in hg_list:\n",
    "    incidence_1 = hg.incidence_matrix()\n",
    "    incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "    incidence_1_list.append(incidence_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the DHGNNLayer class, we create a neural network with a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:55.343005145Z",
     "start_time": "2023-06-01T16:14:55.339481459Z"
    }
   },
   "outputs": [],
   "source": [
    "channels_edge = x_1_list[0].shape[1]\n",
    "channels_node = dataset[0].x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:56.033274119Z",
     "start_time": "2023-06-01T16:14:56.029056913Z"
    }
   },
   "outputs": [],
   "source": [
    "class DHGNNNN(torch.nn.Module):\n",
    "    \"\"\"Neural network implementation of Template for hypergraph classification.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    channels_edge : int\n",
    "        Dimension of edge features\n",
    "    channels_node : int\n",
    "        Dimension of node features\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels_edge, channels_node):\n",
    "        super().__init__()\n",
    "        self.layer = DHGNNLayer(\n",
    "                    in_channels=channels_edge,\n",
    "                    intermediate_channels=channels_node\n",
    "                )\n",
    "\n",
    "    def forward(self, x_1, incidence_1):\n",
    "        \"\"\"Forward computation through layers, then linear layer, then global max pooling.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x_1 : tensor\n",
    "            shape = [n_edges, channels_edge]\n",
    "            Edge features.\n",
    "\n",
    "        incidence_1 : tensor\n",
    "            shape = [n_nodes, n_edges]\n",
    "            Boundary matrix of rank 1.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [1]\n",
    "            Label assigned to whole complex.\n",
    "        \"\"\"\n",
    "        x_1 = self.layer(x_1, incidence_1)\n",
    "        #pooled_x = torch.max(x_1, dim=0)[0]\n",
    "        #return torch.sigmoid(self.linear(pooled_x))[0]\n",
    "        return x_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, the loss, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "model = DHGNNNN(channels_edge, channels_node)\n",
    "model = model.to(device)\n",
    "crit = torch.nn.BCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:59.046068930Z",
     "start_time": "2023-06-01T16:14:59.037648626Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_1_train, x_1_test = train_test_split(x_1_list, test_size=test_size, shuffle=False)\n",
    "incidence_1_train, incidence_1_test = train_test_split(\n",
    "    incidence_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(y_list, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.6087 Train_acc: 0.6875\n",
      "Epoch: 2 loss: 0.5492 Train_acc: 0.8125\n",
      "Test_acc: 0.7500\n",
      "Epoch: 3 loss: 0.5097 Train_acc: 0.7500\n",
      "Epoch: 4 loss: 0.4999 Train_acc: 0.7500\n",
      "Test_acc: 0.7500\n",
      "Epoch: 5 loss: 0.4986 Train_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 5\n",
    "threshold_probability_positive_class = 0.5\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    num_samples = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for x_1, incidence_1, y in zip(x_1_train, incidence_1_train, y_train):\n",
    "        x_1, incidence_1, y = x_1.float().to(device), incidence_1.float().to(device), torch.tensor(y, dtype=torch.float).to(device)\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_1, incidence_1)\n",
    "        loss = crit(y_hat, y)\n",
    "        correct += ((y_hat > threshold_probability_positive_class) == y.bool()).sum().item()\n",
    "        num_samples += 1\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    train_acc = correct / num_samples\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {train_acc:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            num_samples = 0\n",
    "            correct = 0\n",
    "            for x_1, incidence_1, y in zip(x_1_test, incidence_1_test, y_test):\n",
    "                x_1, incidence_1, y = x_1.float().to(device), incidence_1.float().to(device), torch.tensor(y, dtype=torch.float).to(device)\n",
    "                y_hat = model(x_1, incidence_1)\n",
    "                correct += ((y_hat > threshold_probability_positive_class) == y.bool()).sum().item()\n",
    "                num_samples += 1\n",
    "            test_acc = correct / num_samples\n",
    "            print(f\"Test_acc: {test_acc:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
